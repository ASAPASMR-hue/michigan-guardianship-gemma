name: Quality Assurance Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
  USE_SMALL_MODEL: 'true'  # Use small models for CI
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  CI_MODE: 'true'

jobs:
  quality-checks:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black mypy
    
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics
    
    - name: Format check with black
      run: |
        black --check --diff .
    
    - name: Type check with mypy
      run: |
        mypy scripts/ --ignore-missing-imports
      continue-on-error: true
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=scripts --cov-report=xml
      continue-on-error: true
    
    - name: Validate YAML configurations
      run: |
        python -c "
        import yaml
        import sys
        from pathlib import Path
        
        failed = False
        for yaml_file in Path('config').glob('*.yaml'):
            try:
                with open(yaml_file) as f:
                    yaml.safe_load(f)
                print(f'✓ {yaml_file} is valid')
            except Exception as e:
                print(f'✗ {yaml_file} is invalid: {e}')
                failed = True
        
        for yaml_file in Path('.').glob('**/*.yaml'):
            if '.github' not in str(yaml_file):
                try:
                    with open(yaml_file) as f:
                        yaml.safe_load(f)
                    print(f'✓ {yaml_file} is valid')
                except Exception as e:
                    print(f'✗ {yaml_file} is invalid: {e}')
                    failed = True
        
        sys.exit(1 if failed else 0)
        "
    
    - name: Check file integrity
      run: |
        # Verify critical files exist
        FILES=(
          "config/chunking.yaml"
          "config/embedding.yaml"
          "config/retrieval_pipeline.yaml"
          "config/validator.yaml"
          "constants/genesee.yaml"
          "patterns/out_of_scope.json"
          "scripts/orchestrator.py"
          "rubrics/rubric.yaml"
        )
        
        for file in "${FILES[@]}"; do
          if [ -f "$file" ]; then
            echo "✓ $file exists"
          else
            echo "✗ $file is missing"
            exit 1
          fi
        done

  rubric-validation:
    runs-on: ubuntu-latest
    needs: quality-checks
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run rubric validation
      run: |
        python -c "
        import yaml
        from pathlib import Path
        
        # Load rubric
        with open('rubrics/rubric.yaml') as f:
            rubric = yaml.safe_load(f)
        
        # Validate scoring dimensions sum to 10
        total_weight = 0
        dimensions = ['procedural_accuracy', 'substantive_legal_accuracy', 
                     'actionability', 'mode_effectiveness', 'strategic_caution',
                     'citation_quality', 'harm_prevention']
        
        eval_rubric = rubric.get('evaluation_rubric', {})
        for dim in dimensions:
            weight = eval_rubric.get(dim, {}).get('weight', 0)
            total_weight += weight
            print(f'{dim}: {weight}')
        
        print(f'Total weight: {total_weight}')
        assert abs(total_weight - 10.0) < 0.01, f'Total weight {total_weight} != 10.0'
        print('✓ Rubric weights sum to 10.0')
        "
    
    - name: Validate question complexity tiers
      run: |
        python -c "
        import yaml
        
        with open('rubrics/rubric.yaml') as f:
            rubric = yaml.safe_load(f)
        
        tiers = rubric.get('question_complexity_tiers', {})
        required_tiers = ['simple', 'standard', 'complex', 'crisis']
        
        for tier in required_tiers:
            assert tier in tiers, f'Missing tier: {tier}'
            tier_data = tiers[tier]
            assert 'success_threshold' in tier_data
            assert 'critical_dimensions' in tier_data
            print(f'✓ {tier} tier validated')
        "

  performance-benchmarks:
    runs-on: ubuntu-latest
    needs: quality-checks
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run latency benchmarks
      run: |
        python -c "
        import yaml
        
        # Load retrieval pipeline config
        with open('config/retrieval_pipeline.yaml') as f:
            config = yaml.safe_load(f)
        
        # Validate latency budgets
        tiers = config.get('query_complexity_tiers', {})
        
        for tier_name, tier_config in tiers.items():
            budget = tier_config.get('latency_budget_ms', 0)
            p95 = tier_config.get('latency_p95_ms', 0)
            
            print(f'{tier_name}:')
            print(f'  Budget: {budget}ms')
            print(f'  P95: {p95}ms')
            
            # Ensure P95 > budget
            assert p95 >= budget, f'{tier_name}: P95 {p95} < budget {budget}'
            
            # Ensure within global 2s typical constraint
            assert p95 <= 2500, f'{tier_name}: P95 {p95}ms exceeds 2500ms limit'
        
        print('✓ All latency budgets validated')
        "

  security-scan:
    runs-on: ubuntu-latest
    needs: quality-checks
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run security scan
      uses: pyupio/safety@2.3.5
      with:
        api-key: ${{ secrets.SAFETY_API_KEY }}
      continue-on-error: true
    
    - name: Check for hardcoded secrets
      run: |
        # Simple check for potential secrets
        if grep -r "AIzaSy" --include="*.py" --include="*.yaml" --include="*.json" . 2>/dev/null; then
          echo "WARNING: Potential API key found in code"
          exit 1
        fi
        
        if grep -r "sk-[a-zA-Z0-9]" --include="*.py" --include="*.yaml" --include="*.json" . 2>/dev/null; then
          echo "WARNING: Potential secret key found in code"
          exit 1
        fi
        
        echo "✓ No hardcoded secrets detected"

  phase1-pipeline:
    runs-on: ubuntu-latest
    needs: quality-checks
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache Hugging Face models
      uses: actions/cache@v3
      with:
        path: ~/.cache/huggingface
        key: ${{ runner.os }}-huggingface-small-models
        restore-keys: |
          ${{ runner.os }}-huggingface-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Prepare test data
      run: |
        # Create necessary directories
        mkdir -p data/kb_files data/kb_pdfs logs results
        mkdir -p kb_files kb_pdfs  # Legacy paths
        
        # Create minimal test documents
        echo "# Michigan Minor Guardianship" > data/kb_files/test_guardianship.md
        echo "Filing fee is \$175 in Genesee County" >> data/kb_files/test_guardianship.md
        echo "Hearings are held on Thursdays at 9:00 AM" >> data/kb_files/test_guardianship.md
        echo "Use Form PC 651 for petition" >> data/kb_files/test_guardianship.md
        
        # Create sample evaluation CSV
        echo "id,question_text,category,ground_truth" > data/guardianship_qa_cleaned.csv
        echo "GAP001,What is the filing fee for guardianship?,filing,The filing fee is \$175. Fee waiver available with Form MC 20." >> data/guardianship_qa_cleaned.csv
        echo "GAP002,When are guardianship hearings held?,hearings,Hearings are held on Thursdays at 9:00 AM." >> data/guardianship_qa_cleaned.csv
    
    - name: Run Phase 1 Pipeline
      run: |
        echo "=== Running Phase 1 Pipeline with logging ==="
        make phase1 2>&1 | tee logs/phase1_ci_run.log
    
    - name: Validate ChromaDB creation
      run: |
        if [ -d "chroma_db" ]; then
          echo "✓ ChromaDB created successfully"
          du -sh chroma_db/
        else
          echo "✗ ChromaDB not found"
          exit 1
        fi
    
    - name: Validate evaluation results
      run: |
        if ls results/evaluation_results_*.csv 1> /dev/null 2>&1; then
          echo "✓ Evaluation results generated"
          # Show summary
          python -c "
import pandas as pd
import glob
latest = sorted(glob.glob('results/evaluation_results_*.csv'))[-1]
df = pd.read_csv(latest)
print(f'Total questions: {len(df)}')
print(f'Passed: {df[\"pass\"].sum()} ({df[\"pass\"].mean()*100:.1f}%)')
print(f'Average score: {df[\"overall_score\"].mean():.3f}')
          "
        else
          echo "✗ No evaluation results found"
          exit 1
        fi
    
    - name: Upload artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: phase1-artifacts
        path: |
          results/
          logs/
          project_log.md
    
    - name: Check for critical errors
      run: |
        if grep -E "(CRITICAL|ERROR|Exception|Traceback)" logs/*.log; then
          echo "⚠️ Critical errors found in logs"
          exit 1
        else
          echo "✓ No critical errors in logs"
        fi

  deploy-gate:
    runs-on: ubuntu-latest
    needs: [quality-checks, rubric-validation, performance-benchmarks, security-scan, phase1-pipeline]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Quality gate check
      run: |
        echo "✓ All quality gates passed"
        echo "Ready for deployment"
        
        # In a real deployment, you would:
        # 1. Build and push Docker image
        # 2. Deploy to staging environment
        # 3. Run integration tests
        # 4. Deploy to production with canary